{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d475e592-f510-4082-9625-b491591f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Your HTML string\n",
    "html = '''<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title> Testing Web Page </title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1> Web Scraping </h1>\n",
    "\n",
    "    <p id=\"first_para\">\n",
    "        Let's start learning \n",
    "        <b>Web Scraping</b>\n",
    "    </p>\n",
    "\n",
    "    <p class=\"abc\" id=\"second_para\">\n",
    "        You can read more about BeautifulSoup from \n",
    "        <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\"> here </a>\n",
    "    </p>\n",
    "\n",
    "    <p class=\"abc\">\n",
    "        <a href=\"https://codingninjas.in/\"> Coding Ninjas </a>\n",
    "    </p>\n",
    "</body>\n",
    "</html>\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b726088c-2c95-43f1-afef-d87a25bd9363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Testing Web Page \n",
      "Heading:  Web Scraping \n",
      "First Paragraph: Let's start learning \n",
      "        Web Scraping\n",
      "abc Paragraph 1: You can read more about BeautifulSoup from \n",
      "         here\n",
      "abc Paragraph 2: Coding Ninjas\n",
      "Link 1 Text: here\n",
      "Link 1 URL: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
      "Link 2 Text: Coding Ninjas\n",
      "Link 2 URL: https://codingninjas.in/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Extract title\n",
    "title = soup.title.text\n",
    "print(\"Title:\", title)\n",
    "\n",
    "# Extract heading\n",
    "heading = soup.find(\"h1\").text\n",
    "print(\"Heading:\", heading)\n",
    "\n",
    "# Extract paragraph by ID\n",
    "first_para = soup.find(\"p\", id=\"first_para\").text.strip()\n",
    "print(\"First Paragraph:\", first_para)\n",
    "\n",
    "# Extract all paragraphs with class 'abc'\n",
    "abc_paras = soup.find_all(\"p\", class_=\"abc\")\n",
    "for i, para in enumerate(abc_paras, 1):\n",
    "    print(f\"abc Paragraph {i}:\", para.text.strip())\n",
    "\n",
    "# Extract all links\n",
    "links = soup.find_all(\"a\")\n",
    "for i, link in enumerate(links, 1):\n",
    "    print(f\"Link {i} Text:\", link.text.strip())\n",
    "    print(f\"Link {i} URL:\", link.get(\"href\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
